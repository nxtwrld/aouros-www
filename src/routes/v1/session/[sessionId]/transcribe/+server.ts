import { error, json, type RequestHandler } from "@sveltejs/kit";
import { transcriptionProvider } from "$lib/ai/providers/transcription-abstraction";
import { getSession } from "$lib/session/manager";
import { type AudioChunkMetadata, type TranscriptionSegment } from "$lib/audio/overlap-processor";
import fs from "fs";
import path from "path";

// Load audio transcription config
const configPath = path.join(process.cwd(), "config", "audio-transcription.json");
let audioTranscriptionConfig: any = {};
try {
  const configData = fs.readFileSync(configPath, "utf-8");
  audioTranscriptionConfig = JSON.parse(configData);
} catch (e) {
  console.warn("Could not load audio transcription config, using defaults");
}

export const POST: RequestHandler = async ({
  params,
  request,
  locals: { supabase, safeGetSession, user },
}) => {
  const { session } = await safeGetSession();

  if (!session || !user) {
    error(401, { message: "Unauthorized" });
  }

  const sessionId = params.sessionId!;
  console.log("üéØ TRANSCRIBE: Audio chunk received for session:", sessionId);

  // Retrieve session data to get language and other settings
  const sessionData = getSession(sessionId);
  if (!sessionData) {
    console.error("‚ùå TRANSCRIBE: Session not found:", sessionId);
    error(404, { message: "Session not found" });
  }

  try {
    // Use session language for transcription with fallback to English
    let instructions = {
      lang: sessionData.language || "en",
      translate: sessionData.translate || false,
    };

    const formData = await request.formData();
    const uploadedFile = formData.get("audio") as File;
    const chunkId = formData.get("chunkId") as string;
    const sequenceNumber = formData.get("sequenceNumber") as string;
    const timestamp = formData.get("timestamp") as string;
    const instructionsExtend = formData.get("instructions") as string;
    
    // Extract overlap processing metadata if available
    const chunkMetadata = formData.get("metadata") as string;
    let audioMetadata: AudioChunkMetadata | undefined;
    
    if (chunkMetadata) {
      try {
        audioMetadata = JSON.parse(chunkMetadata) as AudioChunkMetadata;
      } catch (e) {
        console.warn("Failed to parse audio chunk metadata:", e);
      }
    }

    // Parse custom instructions if provided
    if (instructionsExtend) {
      try {
        instructions = Object.assign(
          instructions,
          JSON.parse(instructionsExtend),
        );
      } catch (e) {
        console.warn("Failed to parse transcription instructions:", e);
      }
    }

    // Validate required fields
    if (!uploadedFile) {
      error(400, { message: "No audio file provided" });
    }
    if (!(uploadedFile instanceof File)) {
      error(400, { message: "Invalid file format" });
    }
    if (!uploadedFile.type.includes("audio")) {
      error(400, { message: "Invalid file type - must be audio" });
    }
    if (!chunkId) {
      error(400, { message: "Missing chunkId" });
    }

    console.log("üîÑ TRANSCRIBE: Processing", {
      sessionId: sessionId.substring(0, 8) + "...",
      chunkId: chunkId.substring(0, 8) + "...",
      language: instructions.lang,
      translate: instructions.translate,
      size: `${Math.round(uploadedFile.size / 1024)}KB`,
      hasMetadata: !!audioMetadata,
      sequenceNumber: audioMetadata?.sequenceNumber,
      isTimeoutForced: audioMetadata?.isTimeoutForced,
      overlapDuration: audioMetadata?.overlapDurationMs,
    });

    // Initialize transcription provider and transcribe
    await transcriptionProvider.initialize();
    const transcriptionResult = await transcriptionProvider.transcribeAudioCompatible(
      uploadedFile,
      instructions,
    );
    
    // Create transcription segment for overlap processing
    const transcriptionSegment: TranscriptionSegment = {
      text: transcriptionResult.text || "",
      confidence: (transcriptionResult as any).confidence || 0.8,
      timestamp: audioMetadata?.timestamp || parseInt(timestamp, 10) || Date.now(),
      chunkMetadata: audioMetadata || {
        sequenceNumber: parseInt(sequenceNumber, 10) || 0,
        timestamp: parseInt(timestamp, 10) || Date.now(),
        isTimeoutForced: false,
        overlapDurationMs: 0,
        energyLevel: 0,
      },
      processed: false,
    };
    
    // Process overlaps if enabled and multiple segments available
    let processedTranscription = transcriptionSegment;
    const overlapConfig = audioTranscriptionConfig.transcriptionSettings?.overlapProcessing;
    
    if (overlapConfig?.enabled && audioMetadata?.sequenceNumber && audioMetadata.sequenceNumber > 1) {
      // In a real implementation, you would maintain a session-based segment buffer
      // For now, we'll just log that overlap processing is available
      console.log("üîÑ OVERLAP: Overlap processing enabled for sequence", audioMetadata.sequenceNumber);
      
      // Here you would:
      // 1. Retrieve previous segments for this session
      // 2. Add current segment to the buffer
      // 3. Run overlap detection and merging
      // 4. Update the session's transcript buffer
      // 
      // Example:
      // const sessionSegments = getSessionSegments(sessionId);
      // sessionSegments.push(transcriptionSegment);
      // const mergedResult = overlapProcessor.mergeSegments(sessionSegments);
      // updateSessionTranscript(sessionId, mergedResult);
    }

    // Enhanced response format for session-based transcription with overlap metadata
    const response = {
      success: true,
      sessionId,
      chunkId,
      sequenceNumber: sequenceNumber ? parseInt(sequenceNumber, 10) : undefined,
      timestamp: timestamp ? parseInt(timestamp, 10) : Date.now(),
      transcription: {
        text: processedTranscription.text,
        confidence: processedTranscription.confidence,
        language: instructions.lang,
        duration: (transcriptionResult as any).duration || 0,
        processed: processedTranscription.processed,
      },
      chunkMetadata: audioMetadata,
      overlapProcessing: {
        enabled: overlapConfig?.enabled || false,
        canProcess: !!audioMetadata && audioMetadata.sequenceNumber > 1,
        sequenceNumber: audioMetadata?.sequenceNumber,
        isTimeoutChunk: audioMetadata?.isTimeoutForced || false,
      },
      processing: {
        provider: "whisper", // transcriptionProvider info
        processingTime: Date.now() - (timestamp ? parseInt(timestamp, 10) : Date.now()),
        energyLevel: audioMetadata?.energyLevel,
        overlapDuration: audioMetadata?.overlapDurationMs,
      },
    };

    console.log("‚úÖ TRANSCRIBE: Completed", {
      sessionId: sessionId.substring(0, 8) + "...",
      chunkId: chunkId.substring(0, 8) + "...",
      chars: response.transcription.text.length,
      preview: response.transcription.text.substring(0, 30) + (response.transcription.text.length > 30 ? "..." : ""),
      sequenceNumber: audioMetadata?.sequenceNumber,
      isTimeoutForced: audioMetadata?.isTimeoutForced,
      overlapEnabled: overlapConfig?.enabled,
      confidence: response.transcription.confidence,
    });

    return json(response);
  } catch (err) {
    console.error("‚ùå TRANSCRIBE: Failed to process audio chunk:", {
      sessionId,
      error: err instanceof Error ? err.message : String(err),
    });
    error(500, { message: "Failed to transcribe audio chunk" });
  }
};